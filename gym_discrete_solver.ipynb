{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"my_taxi_v2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6fWqag1TycSq","colab_type":"text"},"source":["# Solution for OpenAI Gym - discreete environments\n","\n","Test cases: 'taxi-v2' and 'CliffWalking-v0'\n","\n","*Author: Nick Ben*"]},{"cell_type":"markdown","metadata":{"id":"9qtEezmrpVUr","colab_type":"text"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"dVoroSf30S-o","colab_type":"code","colab":{}},"source":["import numpy as np\n","from collections import defaultdict, deque\n","import gym\n","import numpy as np\n","import sys\n","import math\n","import matplotlib.pyplot as plt\n","import random\n","\n","# Set plotting options\n","%matplotlib inline\n","plt.style.use('ggplot')\n","np.set_printoptions(precision=3, linewidth=120)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G197bL6GVA0h","colab_type":"text"},"source":["## Helpers"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7wMLByZ9z4MN","colab":{}},"source":["def action_egreedy(Q_state, eps, nA):\n","  if random.random() < eps:\n","    action = random.choice(np.arange(nA))\n","  else:\n","    action = np.random.choice([action_ for action_, value_ in enumerate(Q_state) if value_ == np.max(Q_state)])\n","  return action\n","\n","def run(agent, env, num_episodes=20000, mode='train'):\n","    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n","    window = 100 \n","    avg_rewards = deque(maxlen=num_episodes)\n","    best_avg_reward = -math.inf\n","    samp_rewards = deque(maxlen=window)\n","  \n","    for i_episode in range(1, num_episodes+1):\n","        # Initialize episode\n","        initial_state = env.reset()\n","        action = agent.reset_episode(initial_state, i_episode)\n","        samp_reward = 0\n","        done = False\n","\n","        while not done:\n","            next_state, reward, done, _ = env.step(action)\n","            action = agent.step(next_state, reward, done, mode)\n","            samp_reward += reward\n","        samp_rewards.append(samp_reward)\n","        \n","        # Print episode stats\n","        if i_episode > 100:\n","          avg_reward = np.mean(samp_rewards)\n","          avg_rewards.append(avg_reward)\n","          if avg_reward > best_avg_reward:\n","            best_avg_reward = avg_reward\n","            print(\"\\rEpisode {}/{} || Best average reward {} \".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n","            sys.stdout.flush()\n","        if i_episode == num_episodes: print('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0MIH1-pz9tK","colab_type":"text"},"source":["## Agent"]},{"cell_type":"code","metadata":{"id":"9mTYH_Vsz9M2","colab_type":"code","colab":{}},"source":["class Agent:\n","    \"\"\"Agent that can act on an environment\"\"\"\n","\n","    def __init__(self, env, learning = \"sarsa\", double = False, train = True, alpha=0.1, gamma=1.0,\n","                 epsilon_start=1.0, seed=505):\n","\n","        self.env = env\n","        self.state_size = env.observation_space.n\n","        self.action_size = env.action_space.n\n","        self.seed = np.random.seed(seed)\n","        self.learning = learning\n","        self.double = double\n","        self.alpha = alpha  \n","        self.gamma = gamma  \n","        self.epsilon_start = epsilon_start  \n","        \n","        # Create Q-table\n","        if self.double:\n","          self.q_table_1 = np.zeros((self.state_size, self.action_size))\n","          self.q_table_2 = np.zeros((self.state_size, self.action_size))\n","        else:\n","          self.q_table = np.zeros((self.state_size, self.action_size))\n","\n","    def reset_episode(self, initial_state, step):\n","\n","        # Gradually decrease exploration rate\n","        self.epsilon = self.epsilon_start / (0.5*step + 1)\n","\n","        # Decide initial action\n","        self.last_state = initial_state\n","        \n","        if self.double:\n","          self.last_action = np.argmax(np.mean([self.q_table_1[self.last_state], self.q_table_2[self.last_state]], axis=0))\n","        else:\n","          self.last_action = np.argmax(self.q_table[self.last_state])\n","        \n","        return self.last_action\n","    \n","    def step(self, new_state, reward=None, done=None, mode='train'):\n","        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n","\n","        if mode == 'test':\n","            # Test mode: take greedy action\n","            action = np.argmax(self.q_table[state])\n","        \n","        else:\n","            # Train mode: take a step and return action\n","            \n","            # QL step update \n","            if self.learning == \"q_learning\":\n","              self.q_table[self.last_state, self.last_action] += self.alpha * \\\n","                (reward + self.gamma * max(self.q_table[new_state]) - self.q_table[self.last_state, self.last_action])\n","              new_action = action_egreedy(self.q_table[self.last_state], self.epsilon, self.action_size)\n","                          \n","            # SARSA step update \n","            elif self.learning == \"sarsa\":\n","              new_action = action_egreedy(self.q_table[new_state], self.epsilon, self.action_size)\n","              self.q_table[self.last_state, self.last_action] += self.alpha * \\\n","                (reward + self.gamma * self.q_table[new_state, new_action] - self.q_table[self.last_state, self.last_action])\n","            \n","            # Expected SARSA step update \n","            elif self.learning == \"expected_sarsa\":\n","              self.q_table[self.last_state, self.last_action] += self.alpha * \\\n","                (reward + self.gamma * np.mean(self.q_table[new_state]) - self.q_table[self.last_state, self.last_action])\n","              new_action = action_egreedy(self.q_table[new_state], self.epsilon, self.action_size)\n","            \n","            # Double Sarsa step update \n","            elif self.learning == \"double_sarsa\":\n","              new_action = action_egreedy(np.mean([self.q_table_1[new_state],self.q_table_2[new_state]], axis=0), self.epsilon, self.action_size)\n","              if random.random() < 0.5:\n","                self.q_table_1[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * self.q_table_1[new_state, new_action] - self.q_table_1[self.last_state, self.last_action])\n","              else:\n","                self.q_table_2[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * self.q_table_2[new_state, new_action] - self.q_table_2[self.last_state, self.last_action])\n","            \n","            # Double Expected Sarsa step update \n","            elif self.learning == \"double_expected_sarsa\":\n","              if random.random() < 0.5:\n","                self.q_table_1[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * np.mean(self.q_table_2[new_state]) - self.q_table_1[self.last_state, self.last_action])\n","              else:\n","                self.q_table_2[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * np.mean(self.q_table_1[new_state]) - self.q_table_2[self.last_state, self.last_action])\n","              new_action = action_egreedy(np.mean([self.q_table_1[new_state],self.q_table_2[new_state]], axis=0), self.epsilon, self.action_size)\n","            \n","            # Double QL step update \n","            elif self.learning == \"double_q_learning\":\n","              if random.random() < 0.5:\n","                self.q_table_1[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * self.q_table_2[new_state, np.argmax(self.q_table_1[new_state])] - self.q_table_1[self.last_state, self.last_action])\n","              else:\n","                self.q_table_2[self.last_state, self.last_action] += self.alpha * (reward + self.gamma * self.q_table_1[new_state, np.argmax(self.q_table_2[new_state])] - self.q_table_2[self.last_state, self.last_action])\n","              new_action = action_egreedy(np.mean([self.q_table_1[self.last_state],self.q_table_2[self.last_state]], axis=0), self.epsilon, self.action_size)\n","            \n","            else:\n","              raise ValueError('Learning algorithm not supported')\n","            \n","            #rollout state and action\n","            self.last_state = new_state\n","            self.last_action = new_action\n","            return new_action\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kbOws55F1H4n","colab_type":"text"},"source":["## Runs"]},{"cell_type":"markdown","metadata":{"id":"XE1ZkVE4Ee6j","colab_type":"text"},"source":["### Env: 'taxi-v2'"]},{"cell_type":"markdown","metadata":{"id":"jFCqqXyZQ7Q4","colab_type":"text"},"source":["#### Double Q-Learning"]},{"cell_type":"code","metadata":{"id":"3SpNiAx3RHan","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"5937bd58-a865-4d8a-ab04-c5f4532937b3","executionInfo":{"status":"ok","timestamp":1557327241779,"user_tz":-60,"elapsed":411789,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"double_q_learning\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = True,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Episode 106987/200000 || Best average reward -10.04 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lrE8vqCCaywf","colab_type":"text"},"source":["#### Double Sarsa"]},{"cell_type":"code","metadata":{"id":"rLfvLk9gayV7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"55c1a5c2-0f4f-4271-ba6f-6ef5d834281b","executionInfo":{"status":"ok","timestamp":1557327419843,"user_tz":-60,"elapsed":589825,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"double_sarsa\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = True,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Episode 83595/200000 || Best average reward 9.36 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"awsnsQpGy8IN","colab_type":"text"},"source":["#### Sarsa"]},{"cell_type":"code","metadata":{"id":"3jOn6qdy6MGN","colab_type":"code","outputId":"129af76c-86e2-4d30-c5d6-876488e38b7d","executionInfo":{"status":"ok","timestamp":1557327544233,"user_tz":-60,"elapsed":714178,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"sarsa\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Episode 106833/200000 || Best average reward 9.32 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S04I7lRfpiXu","colab_type":"text"},"source":["#### Double Expected Sarsa"]},{"cell_type":"code","metadata":{"id":"JeYoPCyopl6x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"ecfadc26-d929-41bf-bf9a-9637b5954e3a","executionInfo":{"status":"ok","timestamp":1557327758817,"user_tz":-60,"elapsed":928743,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"double_expected_sarsa\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = True,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Episode 178868/200000 || Best average reward 9.37 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kdtQlXkNzAmC","colab_type":"text"},"source":["# Expected Sarsa : *WINNER*"]},{"cell_type":"code","metadata":{"id":"J8xnjIsApUSS","colab_type":"code","outputId":"539b70d7-07a7-424c-8fe7-a2850c3f67bb","executionInfo":{"status":"ok","timestamp":1557327914241,"user_tz":-60,"elapsed":1084148,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"expected_sarsa\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Episode 153537/200000 || Best average reward 9.73 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QkZsYr-A0j2J","colab_type":"text"},"source":["#### Q-learning"]},{"cell_type":"code","metadata":{"id":"jFlo-Eqx0I0j","colab_type":"code","outputId":"4d7e935e-187b-4c64-dbd4-4918253100d3","executionInfo":{"status":"ok","timestamp":1557328134399,"user_tz":-60,"elapsed":1304293,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["ENV_NAME = \"Taxi-v2\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.5\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"q_learning\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Episode 188756/200000 || Best average reward -7.96 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svfNz4lR474E","colab_type":"text"},"source":["### Env: 'CliffWalking-v0'"]},{"cell_type":"code","metadata":{"id":"BmpIzkk83RuK","colab_type":"code","colab":{}},"source":["ENV_NAME = 'CliffWalking-v0'\n","EPISODES_TRAIN = 2000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","\n","env = gym.make(ENV_NAME)\n","agent_ql = Agent(env, \n","              learning = \"q_learning\",\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","agent_sarsa = Agent(env, \n","              learning = \"sarsa\",\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","agent_expsarsa = Agent(env, \n","              learning = \"expected_sarsa\",\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X93Pqxnv3VVd","colab_type":"code","outputId":"7b3cba59-c540-4c15-984f-8c288a22a19f","executionInfo":{"status":"ok","timestamp":1557328136287,"user_tz":-60,"elapsed":1306171,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["run(agent_sarsa, env, num_episodes = EPISODES_TRAIN)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Episode 1131/2000 || Best average reward -15.0 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9k-ouBz4DNeO","colab_type":"code","outputId":"2e61e96c-13e5-47f8-c476-b865ef6e1e65","executionInfo":{"status":"ok","timestamp":1557328138699,"user_tz":-60,"elapsed":1308571,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["run(agent_expsarsa, env, num_episodes = EPISODES_TRAIN)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Episode 1297/2000 || Best average reward -15.0 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qv_LgVXgDTI8","colab_type":"code","outputId":"a12dc0ac-1481-4904-9af6-9d91abe87523","executionInfo":{"status":"ok","timestamp":1557328452444,"user_tz":-60,"elapsed":1622303,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["run(agent_ql, env, num_episodes = EPISODES_TRAIN)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Episode 181/2000 || Best average reward -826.72 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nhyB12Sfwt4y","colab_type":"text"},"source":["## FrozenLake-v0"]},{"cell_type":"code","metadata":{"id":"RWdpYSPnDV02","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"84f87ad1-ab81-433f-c45a-ceb003b45c36","executionInfo":{"status":"ok","timestamp":1557328586000,"user_tz":-60,"elapsed":1755838,"user":{"displayName":"Nick Ben","photoUrl":"https://lh5.googleusercontent.com/-k6tFR98egZE/AAAAAAAAAAI/AAAAAAAACZY/ui6e4H6DiPg/s64/photo.jpg","userId":"02195147717852497268"}}},"source":["ENV_NAME = \"FrozenLake-v0\"\n","EPISODES_TRAIN = 200000\n","ALPHA = 0.1\n","GAMMA = 1.0\n","EPS_START = 1.0\n","SEED = 505\n","LEARN_ALGO = \"sarsa\"\n","\n","env = gym.make(ENV_NAME)\n","agent = Agent(env, \n","              learning = LEARN_ALGO,\n","              double = False,\n","              train = True,\n","              alpha = ALPHA,\n","              gamma = GAMMA,\n","              epsilon_start = EPS_START,\n","              seed = SEED)\n","\n","run(agent, env, num_episodes = EPISODES_TRAIN)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Episode 107665/200000 || Best average reward 0.38 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xtf3g0DzGoyn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}